# -*- coding: utf-8 -*-
"""
Vision Connector - Ollama è¦–è¦ºæ¨¡å‹é€£æ¥å™¨
æ”¯æ´ Llava, Moondream, Qwen-VL ç­‰å¤šç¨®æ¨¡å‹
"""

import base64
import requests
from io import BytesIO
from typing import Optional, Dict, Any, Union
from pathlib import Path
import numpy as np
import cv2
from PIL import Image


class VisionConnectorError(Exception):
    """è¦–è¦ºé€£æ¥å™¨éŒ¯èª¤"""
    pass


class VisionConnector:
    """
    é€šç”¨è¦–è¦ºæ¨¡å‹é€£æ¥å™¨
    
    ç‰¹æ€§:
    - æ”¯æ´åœ–ç‰‡è·¯å¾‘ã€PIL Imageã€numpy array
    - è‡ªå‹• base64 ç·¨ç¢¼
    - VRAM å„ªåŒ–ï¼ˆkeep_alive æ§åˆ¶ï¼‰
    - éŒ¯èª¤è™•ç†èˆ‡é‡è©¦
    """
    
    DEFAULT_OPTIONS = {
        "temperature": 0.3,  # é™ä½éš¨æ©Ÿæ€§
        "num_predict": 150,  # é™åˆ¶è¼¸å‡ºé•·åº¦
        "num_ctx": 2048,     # ä¸Šä¸‹æ–‡é•·åº¦
    }
    
    def __init__(
        self, 
        base_url: str = "http://localhost:11434",
        model: str = "qwen3-vl-4b", # âœ… Updated default
        timeout: int = 60 # âœ… Increased for larger models
    ):
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.timeout = timeout
        self.generate_url = f"{self.base_url}/api/generate"
        print(f"ğŸ”§ [VisionConnector] Initialized with model: {self.model}, timeout: {self.timeout}s")

    def _is_qwen_vl(self) -> bool:
        """æª¢æ¸¬æ˜¯å¦ç‚º Qwen-VL ç³»åˆ—æ¨¡å‹"""
        return "qwen" in self.model.lower() and "vl" in self.model.lower()
        
    def _image_to_base64(
        self, 
        image: Union[str, Path, Image.Image, np.ndarray]
    ) -> str:
        """
        å°‡åœ–ç‰‡è½‰æ›ç‚º base64 å­—ä¸²
        
        Args:
            image: åœ–ç‰‡ï¼ˆè·¯å¾‘ã€PIL Image æˆ– numpy arrayï¼‰
        
        Returns:
            str: base64 ç·¨ç¢¼å­—ä¸²
        """
        try:
            # è™•ç†æª”æ¡ˆè·¯å¾‘
            if isinstance(image, (str, Path)):
                with open(image, "rb") as f:
                    return base64.b64encode(f.read()).decode("utf-8")
            
            # è™•ç† PIL Image
            elif isinstance(image, Image.Image):
                buffered = BytesIO()
                image.save(buffered, format="PNG")
                return base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            # è™•ç† numpy array (OpenCV/mss æ ¼å¼)
            elif isinstance(image, np.ndarray):
                # ç¢ºä¿æ˜¯ RGB æ ¼å¼
                if len(image.shape) == 3 and image.shape[2] == 4:  # RGBA
                    image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)
                elif len(image.shape) == 3 and image.shape[2] == 3:  # BGR
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                pil_img = Image.fromarray(image)
                buffered = BytesIO()
                pil_img.save(buffered, format="PNG")
                return base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„åœ–ç‰‡é¡å‹: {type(image)}")
                
        except Exception as e:
            raise VisionConnectorError(f"åœ–ç‰‡ç·¨ç¢¼å¤±æ•—: {str(e)}")
    
    def analyze_image(
        self,
        image: Union[str, Path, Image.Image, np.ndarray],
        prompt: str = "Describe this image concisely in English. Focus on main activities, text on screen, or significant events.",
        stream: bool = False,
        keep_alive: str = "5m",  # Keep loaded for 5 minutes
        custom_options: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        åˆ†æåœ–ç‰‡å…§å®¹
        
        Args:
            image: åœ–ç‰‡è¼¸å…¥
            prompt: åˆ†ææç¤ºè©
            stream: æ˜¯å¦ä½¿ç”¨ä¸²æµæ¨¡å¼ï¼ˆé è¨­ Falseï¼‰
            keep_alive: æ¨¡å‹ä¿æŒè¼‰å…¥æ™‚é–“
            custom_options: è‡ªè¨‚ Ollama åƒæ•¸
        
        Returns:
            str: æ¨¡å‹çš„æè¿°æ–‡å­—
        
        Raises:
            VisionConnectorError: ç•¶è«‹æ±‚å¤±æ•—æ™‚
        """
        try:
            # æº–å‚™ base64 åœ–ç‰‡
            image_b64 = self._image_to_base64(image)
            
            # åˆä½µé¸é …
            options = self.DEFAULT_OPTIONS.copy()
            if custom_options:
                options.update(custom_options)
            
            # ğŸ”¥ æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ API ç«¯é»å’Œæ ¼å¼
            if self._is_qwen_vl():
                # Qwen-VL ä½¿ç”¨ /api/chat ç«¯é» (Optimized for stability)
                api_endpoint = f"{self.base_url}/api/chat"
                
                # Use more deterministic options for VL models to prevent loops
                vl_options = options.copy()
                vl_options.update({
                    "temperature": 0.1,
                    "num_predict": 60,
                    "top_k": 20,
                    "repeat_penalty": 1.2
                })
                
                payload = {
                    "model": self.model,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt,
                            "images": [image_b64]
                        }
                    ],
                    "stream": stream,
                    "keep_alive": keep_alive,
                    "options": vl_options
                }
            else:
                # Moondream/å…¶ä»–æ¨¡å‹ä½¿ç”¨ /api/generate
                api_endpoint = self.generate_url
                payload = {
                    "model": self.model,
                    "prompt": prompt,
                    "images": [image_b64],
                    "stream": stream,
                    "keep_alive": keep_alive,
                    "options": options
                }

            # Debug logging
            print(f"[VisionConnector Debug] API Endpoint: {api_endpoint}")
            print(f"[VisionConnector Debug] Model: {self.model}")
            print(f"[VisionConnector Debug] Prompt: {prompt[:100]}...")
            print(f"[VisionConnector Debug] Image size: {len(image_b64)} bytes (base64)")
            print(f"[VisionConnector Debug] Using {'chat' if self._is_qwen_vl() else 'generate'} API")

            # ç™¼é€è«‹æ±‚
            response = requests.post(
                api_endpoint,
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            
            # è§£æå›æ‡‰
            data = response.json()
            
            # ğŸ”¥ æ ¹æ“š API é¡å‹æå– response
            if self._is_qwen_vl():
                # chat API æ ¼å¼
                message = data.get("message", {})
                response_text = message.get("content", "").strip()
                
                # Debug: Check for thinking loop/empty content
                if not response_text and "thinking" in message:
                    thought = message.get("thinking", "")
                    print(f"âš ï¸ [VisionConnector] Model trapped in thought loop: {thought[:100]}...")
                    # Fallback: Return a generic signal
                    return "A computer screen with active windows."
            else:
                # generate API æ ¼å¼
                response_text = data.get("response", "").strip()

            # Debug logging
            print(f"[VisionConnector Debug] Raw response length: {len(response_text)}")
            print(f"[VisionConnector Debug] Response preview: {response_text[:200]}")
            
            if not response_text:
                print(f"âš ï¸ [VisionConnector] Model {self.model} returned empty response!")
                print(f"[VisionConnector Debug] Full API response: {data}")
                # Fallback for empty responses
                if self._is_qwen_vl():
                    return "The screen shows various applications and content."
            
            return response_text
                
        except requests.exceptions.Timeout:
            raise VisionConnectorError(
                f"è«‹æ±‚è¶…æ™‚ï¼ˆ{self.timeout}sï¼‰ã€‚æ¨¡å‹ {self.model} å¯èƒ½å°šæœªè¼‰å…¥æˆ–æ¨ç†æ™‚é–“éé•·"
            )
        except requests.exceptions.ConnectionError:
            raise VisionConnectorError(
                f"ç„¡æ³•é€£æ¥åˆ° Ollama ({self.base_url})ã€‚è«‹ç¢ºèªæœå‹™å·²å•Ÿå‹•"
            )
        except requests.exceptions.HTTPError as e:
            raise VisionConnectorError(f"HTTP éŒ¯èª¤: {e.response.status_code} - {e.response.text}")
        except Exception as e:
            raise VisionConnectorError(f"æœªçŸ¥éŒ¯èª¤: {str(e)}")
    
    def test_connection(self) -> bool:
        """
        æ¸¬è©¦èˆ‡ Ollama çš„é€£æ¥
        
        Returns:
            bool: é€£æ¥æˆåŠŸè¿”å› True
        """
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            return response.status_code == 200
        except:
            return False


# === ä½¿ç”¨ç¯„ä¾‹ ===
if __name__ == "__main__":
    import mss
    
    # åˆå§‹åŒ–é€£æ¥å™¨
    connector = VisionConnector()
    
    # æ¸¬è©¦é€£æ¥
    if not connector.test_connection():
        print("âŒ ç„¡æ³•é€£æ¥åˆ° Ollamaï¼Œè«‹ç¢ºèªæœå‹™å·²å•Ÿå‹•")
        exit(1)
    
    print("âœ… Ollama é€£æ¥æˆåŠŸ")
    
    # æˆªå–ç•¶å‰è¢å¹•
    with mss.mss() as sct:
        monitor = sct.monitors[1]
        screenshot = sct.grab(monitor)
        img_array = np.array(screenshot)
    
    print("ğŸ“¸ è¢å¹•å·²æˆªå–ï¼Œæ­£åœ¨åˆ†æ...")
    
    try:
        # åˆ†æåœ–ç‰‡
        description = connector.analyze_image(
            image=img_array,
            prompt="What's on this screen? Mention any text, applications, or activities.",
            keep_alive="0s"  # ç«‹å³é‡‹æ”¾ VRAM
        )
        
        print(f"\nğŸ” {connector.model} çš„è§€å¯Ÿ:\n{description}")
        
    except VisionConnectorError as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
